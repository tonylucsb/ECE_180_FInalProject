{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6c3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0db91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf137a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 1, 384])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_st = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_train = model_st.encode(train_df[\"query\"].tolist(), convert_to_tensor=True)\n",
    "X_val = model_st.encode(val_df[\"query\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "y_train = torch.tensor(train_df[\"carb\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "y_val = torch.tensor(val_df[\"carb\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "X_train = X_train.unsqueeze(1)  # (N, 1, 384)\n",
    "X_val = X_val.unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe825b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=384, hidden_size=128, num_layers=3, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)          # x: (batch, seq_len, input_size)\n",
    "        out = out[:, -1, :]            # Get last time step\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87a08698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 478781.0383, Train RMSE: 43.76\n",
      "Epoch 2, Train Loss: 411399.3261, Train RMSE: 40.57\n",
      "Epoch 3, Train Loss: 385616.9707, Train RMSE: 39.27\n",
      "Epoch 4, Train Loss: 369906.6596, Train RMSE: 38.47\n",
      "Epoch 5, Train Loss: 359272.4746, Train RMSE: 37.91\n",
      "Epoch 6, Train Loss: 348966.3570, Train RMSE: 37.36\n",
      "Epoch 7, Train Loss: 339618.0681, Train RMSE: 36.86\n",
      "Epoch 8, Train Loss: 329203.4835, Train RMSE: 36.29\n",
      "Epoch 9, Train Loss: 320694.2627, Train RMSE: 35.82\n",
      "Epoch 10, Train Loss: 310047.8438, Train RMSE: 35.22\n",
      "Epoch 11, Train Loss: 300704.5149, Train RMSE: 34.68\n",
      "Epoch 12, Train Loss: 294724.4025, Train RMSE: 34.34\n",
      "Epoch 13, Train Loss: 287573.5440, Train RMSE: 33.92\n",
      "Epoch 14, Train Loss: 278916.2472, Train RMSE: 33.40\n",
      "Epoch 15, Train Loss: 274414.1807, Train RMSE: 33.13\n",
      "Epoch 16, Train Loss: 266313.1424, Train RMSE: 32.64\n",
      "Epoch 17, Train Loss: 261228.8691, Train RMSE: 32.33\n",
      "Epoch 18, Train Loss: 255300.2610, Train RMSE: 31.96\n",
      "Epoch 19, Train Loss: 247892.6529, Train RMSE: 31.49\n",
      "Epoch 20, Train Loss: 250276.6594, Train RMSE: 31.64\n",
      "Epoch 21, Train Loss: 241364.5797, Train RMSE: 31.07\n",
      "Epoch 22, Train Loss: 234988.5073, Train RMSE: 30.66\n",
      "Epoch 23, Train Loss: 231971.0005, Train RMSE: 30.46\n",
      "Epoch 24, Train Loss: 228396.7275, Train RMSE: 30.23\n",
      "Epoch 25, Train Loss: 224615.5848, Train RMSE: 29.97\n",
      "Epoch 26, Train Loss: 216578.2985, Train RMSE: 29.43\n",
      "Epoch 27, Train Loss: 213392.9835, Train RMSE: 29.22\n",
      "Epoch 28, Train Loss: 206900.5552, Train RMSE: 28.77\n",
      "Epoch 29, Train Loss: 200527.0002, Train RMSE: 28.32\n",
      "Epoch 30, Train Loss: 200774.6027, Train RMSE: 28.34\n",
      "Epoch 31, Train Loss: 192692.7574, Train RMSE: 27.76\n",
      "Epoch 32, Train Loss: 189390.9296, Train RMSE: 27.52\n",
      "Epoch 33, Train Loss: 187481.3811, Train RMSE: 27.38\n",
      "Epoch 34, Train Loss: 183683.3885, Train RMSE: 27.11\n",
      "Epoch 35, Train Loss: 180254.4893, Train RMSE: 26.85\n",
      "Epoch 36, Train Loss: 178485.3046, Train RMSE: 26.72\n",
      "Epoch 37, Train Loss: 173697.2898, Train RMSE: 26.36\n",
      "Epoch 38, Train Loss: 171262.2628, Train RMSE: 26.17\n",
      "Epoch 39, Train Loss: 169691.8444, Train RMSE: 26.05\n",
      "Epoch 40, Train Loss: 162447.5934, Train RMSE: 25.49\n",
      "Epoch 41, Train Loss: 160061.8956, Train RMSE: 25.30\n",
      "Epoch 42, Train Loss: 157023.9047, Train RMSE: 25.06\n",
      "Epoch 43, Train Loss: 156716.7866, Train RMSE: 25.04\n",
      "Epoch 44, Train Loss: 155347.8133, Train RMSE: 24.93\n",
      "Epoch 45, Train Loss: 147848.5809, Train RMSE: 24.32\n",
      "Epoch 46, Train Loss: 150063.9198, Train RMSE: 24.50\n",
      "Epoch 47, Train Loss: 145307.9356, Train RMSE: 24.11\n",
      "Epoch 48, Train Loss: 133475.8196, Train RMSE: 23.11\n",
      "Epoch 49, Train Loss: 143726.5777, Train RMSE: 23.98\n",
      "Epoch 50, Train Loss: 129916.8289, Train RMSE: 22.80\n",
      "Epoch 51, Train Loss: 132605.1988, Train RMSE: 23.03\n",
      "Epoch 52, Train Loss: 134908.1152, Train RMSE: 23.23\n",
      "Epoch 53, Train Loss: 121793.5486, Train RMSE: 22.07\n",
      "Epoch 54, Train Loss: 124167.9006, Train RMSE: 22.29\n",
      "Epoch 55, Train Loss: 120318.5768, Train RMSE: 21.94\n",
      "Epoch 56, Train Loss: 123897.7534, Train RMSE: 22.26\n",
      "Epoch 57, Train Loss: 119424.1332, Train RMSE: 21.86\n",
      "Epoch 58, Train Loss: 111214.9395, Train RMSE: 21.09\n",
      "Epoch 59, Train Loss: 115058.4998, Train RMSE: 21.45\n",
      "Epoch 60, Train Loss: 108583.0273, Train RMSE: 20.84\n",
      "Epoch 61, Train Loss: 112017.1003, Train RMSE: 21.17\n",
      "Epoch 62, Train Loss: 108250.8354, Train RMSE: 20.81\n",
      "Epoch 63, Train Loss: 116095.4787, Train RMSE: 21.55\n",
      "Epoch 64, Train Loss: 107801.3583, Train RMSE: 20.77\n",
      "Epoch 65, Train Loss: 101342.6459, Train RMSE: 20.13\n",
      "Epoch 66, Train Loss: 98781.6533, Train RMSE: 19.88\n",
      "Epoch 67, Train Loss: 103329.9632, Train RMSE: 20.33\n",
      "Epoch 68, Train Loss: 105065.2583, Train RMSE: 20.50\n",
      "Epoch 69, Train Loss: 97401.0669, Train RMSE: 19.74\n",
      "Epoch 70, Train Loss: 95412.7995, Train RMSE: 19.54\n",
      "Epoch 71, Train Loss: 92001.2492, Train RMSE: 19.18\n",
      "Epoch 72, Train Loss: 95654.8089, Train RMSE: 19.56\n",
      "Epoch 73, Train Loss: 92936.3494, Train RMSE: 19.28\n",
      "Epoch 74, Train Loss: 92904.1014, Train RMSE: 19.28\n",
      "Epoch 75, Train Loss: 87467.7870, Train RMSE: 18.70\n",
      "Epoch 76, Train Loss: 91384.6245, Train RMSE: 19.12\n",
      "Epoch 77, Train Loss: 93882.0484, Train RMSE: 19.38\n",
      "Epoch 78, Train Loss: 89022.2544, Train RMSE: 18.87\n",
      "Epoch 79, Train Loss: 87092.2562, Train RMSE: 18.66\n",
      "Epoch 80, Train Loss: 90558.7486, Train RMSE: 19.03\n",
      "Epoch 81, Train Loss: 83745.0048, Train RMSE: 18.30\n",
      "Epoch 82, Train Loss: 79465.8718, Train RMSE: 17.83\n",
      "Epoch 83, Train Loss: 79545.3192, Train RMSE: 17.84\n",
      "Epoch 84, Train Loss: 79468.7786, Train RMSE: 17.83\n",
      "Epoch 85, Train Loss: 80096.2058, Train RMSE: 17.90\n",
      "Epoch 86, Train Loss: 77081.1428, Train RMSE: 17.56\n",
      "Epoch 87, Train Loss: 87829.8764, Train RMSE: 18.74\n",
      "Epoch 88, Train Loss: 76320.8648, Train RMSE: 17.47\n",
      "Epoch 89, Train Loss: 74866.9236, Train RMSE: 17.31\n",
      "Epoch 90, Train Loss: 76897.4246, Train RMSE: 17.54\n",
      "Epoch 91, Train Loss: 75076.6452, Train RMSE: 17.33\n",
      "Epoch 92, Train Loss: 72478.5420, Train RMSE: 17.03\n",
      "Epoch 93, Train Loss: 76577.0007, Train RMSE: 17.50\n",
      "Epoch 94, Train Loss: 79749.6531, Train RMSE: 17.86\n",
      "Epoch 95, Train Loss: 77203.2557, Train RMSE: 17.57\n",
      "Epoch 96, Train Loss: 77630.4488, Train RMSE: 17.62\n",
      "Epoch 97, Train Loss: 72149.6020, Train RMSE: 16.99\n",
      "Epoch 98, Train Loss: 66330.1010, Train RMSE: 16.29\n",
      "Epoch 99, Train Loss: 71523.6876, Train RMSE: 16.91\n",
      "Epoch 100, Train Loss: 68921.4728, Train RMSE: 16.60\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = LSTMModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        train_preds.append(pred.detach())\n",
    "        train_targets.append(yb)\n",
    "\n",
    "    train_preds = torch.cat(train_preds)\n",
    "    train_targets = torch.cat(train_targets)\n",
    "    train_rmse = math.sqrt(F.mse_loss(train_preds, train_targets).item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Train RMSE: {train_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311ac6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 19.47\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for xb, yb in val_loader:\n",
    "        y_pred = model(xb)\n",
    "        preds.append(y_pred)\n",
    "        targets.append(yb)\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    rmse = math.sqrt(F.mse_loss(preds, targets).item())\n",
    "    print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "\n",
    "    #18.44 -> hidden_size=128, num_layers=3, dropout=0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76b324bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_val).squeeze().numpy()\n",
    "\n",
    "# Add prediction column and save\n",
    "test_df[\"carb\"] = preds\n",
    "test_df.to_csv(\"test_with_predictions_transformer_lstm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
