{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6c3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0db91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf137a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 1, 384])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_st = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_train = model_st.encode(train_df[\"query\"].tolist(), convert_to_tensor=True)\n",
    "X_val = model_st.encode(val_df[\"query\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "y_train = torch.tensor(train_df[\"carb\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "y_val = torch.tensor(val_df[\"carb\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "X_train = X_train.unsqueeze(1)  # (N, 1, 384)\n",
    "X_val = X_val.unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe825b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=384, hidden_size=128, num_layers=2, dropout=0.4):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)          # x: (batch, seq_len, input_size)\n",
    "        out = out[:, -1, :]            # Get last time step\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87a08698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 481870.2575, Train RMSE: 43.90\n",
      "Epoch 2, Train Loss: 416338.1710, Train RMSE: 40.81\n",
      "Epoch 3, Train Loss: 387644.6107, Train RMSE: 39.38\n",
      "Epoch 4, Train Loss: 370881.0496, Train RMSE: 38.52\n",
      "Epoch 5, Train Loss: 357836.3816, Train RMSE: 37.83\n",
      "Epoch 6, Train Loss: 344904.0132, Train RMSE: 37.14\n",
      "Epoch 7, Train Loss: 334811.2111, Train RMSE: 36.60\n",
      "Epoch 8, Train Loss: 324500.1559, Train RMSE: 36.03\n",
      "Epoch 9, Train Loss: 316150.6721, Train RMSE: 35.56\n",
      "Epoch 10, Train Loss: 304870.2886, Train RMSE: 34.92\n",
      "Epoch 11, Train Loss: 295413.9996, Train RMSE: 34.38\n",
      "Epoch 12, Train Loss: 286553.0310, Train RMSE: 33.86\n",
      "Epoch 13, Train Loss: 279394.9477, Train RMSE: 33.43\n",
      "Epoch 14, Train Loss: 270100.1623, Train RMSE: 32.87\n",
      "Epoch 15, Train Loss: 262113.8207, Train RMSE: 32.38\n",
      "Epoch 16, Train Loss: 259974.2054, Train RMSE: 32.25\n",
      "Epoch 17, Train Loss: 248913.1646, Train RMSE: 31.55\n",
      "Epoch 18, Train Loss: 242563.6898, Train RMSE: 31.15\n",
      "Epoch 19, Train Loss: 238391.4393, Train RMSE: 30.88\n",
      "Epoch 20, Train Loss: 230787.0267, Train RMSE: 30.38\n",
      "Epoch 21, Train Loss: 225103.9898, Train RMSE: 30.01\n",
      "Epoch 22, Train Loss: 217406.7747, Train RMSE: 29.49\n",
      "Epoch 23, Train Loss: 212352.1284, Train RMSE: 29.14\n",
      "Epoch 24, Train Loss: 206461.4626, Train RMSE: 28.74\n",
      "Epoch 25, Train Loss: 199274.7566, Train RMSE: 28.23\n",
      "Epoch 26, Train Loss: 198071.9092, Train RMSE: 28.15\n",
      "Epoch 27, Train Loss: 190556.0394, Train RMSE: 27.61\n",
      "Epoch 28, Train Loss: 185233.7248, Train RMSE: 27.22\n",
      "Epoch 29, Train Loss: 175281.1519, Train RMSE: 26.48\n",
      "Epoch 30, Train Loss: 182709.8490, Train RMSE: 27.03\n",
      "Epoch 31, Train Loss: 172081.1302, Train RMSE: 26.24\n",
      "Epoch 32, Train Loss: 168595.8456, Train RMSE: 25.97\n",
      "Epoch 33, Train Loss: 166279.5051, Train RMSE: 25.79\n",
      "Epoch 34, Train Loss: 157468.5170, Train RMSE: 25.10\n",
      "Epoch 35, Train Loss: 155573.9296, Train RMSE: 24.95\n",
      "Epoch 36, Train Loss: 155128.7908, Train RMSE: 24.91\n",
      "Epoch 37, Train Loss: 150692.6997, Train RMSE: 24.55\n",
      "Epoch 38, Train Loss: 149556.8404, Train RMSE: 24.46\n",
      "Epoch 39, Train Loss: 143144.6401, Train RMSE: 23.93\n",
      "Epoch 40, Train Loss: 141851.7056, Train RMSE: 23.82\n",
      "Epoch 41, Train Loss: 134863.5927, Train RMSE: 23.23\n",
      "Epoch 42, Train Loss: 134866.7334, Train RMSE: 23.23\n",
      "Epoch 43, Train Loss: 139591.7799, Train RMSE: 23.63\n",
      "Epoch 44, Train Loss: 120089.9155, Train RMSE: 21.92\n",
      "Epoch 45, Train Loss: 124420.5202, Train RMSE: 22.31\n",
      "Epoch 46, Train Loss: 122871.0280, Train RMSE: 22.17\n",
      "Epoch 47, Train Loss: 115357.5446, Train RMSE: 21.48\n",
      "Epoch 48, Train Loss: 116153.8983, Train RMSE: 21.55\n",
      "Epoch 49, Train Loss: 113526.9011, Train RMSE: 21.31\n",
      "Epoch 50, Train Loss: 106856.1798, Train RMSE: 20.67\n",
      "Epoch 51, Train Loss: 105951.0369, Train RMSE: 20.59\n",
      "Epoch 52, Train Loss: 103161.1557, Train RMSE: 20.31\n",
      "Epoch 53, Train Loss: 108300.9274, Train RMSE: 20.81\n",
      "Epoch 54, Train Loss: 99672.9465, Train RMSE: 19.97\n",
      "Epoch 55, Train Loss: 102307.5173, Train RMSE: 20.23\n",
      "Epoch 56, Train Loss: 94515.5423, Train RMSE: 19.44\n",
      "Epoch 57, Train Loss: 90759.3778, Train RMSE: 19.05\n",
      "Epoch 58, Train Loss: 94239.8573, Train RMSE: 19.42\n",
      "Epoch 59, Train Loss: 93795.2527, Train RMSE: 19.37\n",
      "Epoch 60, Train Loss: 91019.1296, Train RMSE: 19.08\n",
      "Epoch 61, Train Loss: 87666.8325, Train RMSE: 18.73\n",
      "Epoch 62, Train Loss: 94773.6990, Train RMSE: 19.47\n",
      "Epoch 63, Train Loss: 89691.1714, Train RMSE: 18.94\n",
      "Epoch 64, Train Loss: 82590.7336, Train RMSE: 18.18\n",
      "Epoch 65, Train Loss: 79163.3940, Train RMSE: 17.79\n",
      "Epoch 66, Train Loss: 78484.6228, Train RMSE: 17.72\n",
      "Epoch 67, Train Loss: 76746.7800, Train RMSE: 17.52\n",
      "Epoch 68, Train Loss: 82856.1231, Train RMSE: 18.21\n",
      "Epoch 69, Train Loss: 78834.6627, Train RMSE: 17.76\n",
      "Epoch 70, Train Loss: 73880.2617, Train RMSE: 17.19\n",
      "Epoch 71, Train Loss: 81821.9963, Train RMSE: 18.09\n",
      "Epoch 72, Train Loss: 72966.7543, Train RMSE: 17.08\n",
      "Epoch 73, Train Loss: 73093.2718, Train RMSE: 17.10\n",
      "Epoch 74, Train Loss: 66797.4819, Train RMSE: 16.35\n",
      "Epoch 75, Train Loss: 67098.0843, Train RMSE: 16.38\n",
      "Epoch 76, Train Loss: 70615.5677, Train RMSE: 16.81\n",
      "Epoch 77, Train Loss: 67480.4559, Train RMSE: 16.43\n",
      "Epoch 78, Train Loss: 63973.6405, Train RMSE: 16.00\n",
      "Epoch 79, Train Loss: 66394.9232, Train RMSE: 16.30\n",
      "Epoch 80, Train Loss: 64253.3428, Train RMSE: 16.03\n",
      "Epoch 81, Train Loss: 59693.9839, Train RMSE: 15.45\n",
      "Epoch 82, Train Loss: 59579.4137, Train RMSE: 15.44\n",
      "Epoch 83, Train Loss: 59757.9403, Train RMSE: 15.46\n",
      "Epoch 84, Train Loss: 59458.2835, Train RMSE: 15.42\n",
      "Epoch 85, Train Loss: 58232.2626, Train RMSE: 15.26\n",
      "Epoch 86, Train Loss: 56072.5069, Train RMSE: 14.98\n",
      "Epoch 87, Train Loss: 55751.4747, Train RMSE: 14.93\n",
      "Epoch 88, Train Loss: 56803.3420, Train RMSE: 15.07\n",
      "Epoch 89, Train Loss: 61595.9828, Train RMSE: 15.70\n",
      "Epoch 90, Train Loss: 51525.8887, Train RMSE: 14.36\n",
      "Epoch 91, Train Loss: 54413.2590, Train RMSE: 14.75\n",
      "Epoch 92, Train Loss: 53766.0973, Train RMSE: 14.67\n",
      "Epoch 93, Train Loss: 57130.0330, Train RMSE: 15.12\n",
      "Epoch 94, Train Loss: 64258.0188, Train RMSE: 16.03\n",
      "Epoch 95, Train Loss: 48595.3406, Train RMSE: 13.94\n",
      "Epoch 96, Train Loss: 48326.8139, Train RMSE: 13.90\n",
      "Epoch 97, Train Loss: 49184.0759, Train RMSE: 14.03\n",
      "Epoch 98, Train Loss: 48071.7477, Train RMSE: 13.87\n",
      "Epoch 99, Train Loss: 47744.0028, Train RMSE: 13.82\n",
      "Epoch 100, Train Loss: 45735.8792, Train RMSE: 13.53\n",
      "Epoch 101, Train Loss: 46658.4088, Train RMSE: 13.66\n",
      "Epoch 102, Train Loss: 45233.6649, Train RMSE: 13.45\n",
      "Epoch 103, Train Loss: 45008.3950, Train RMSE: 13.42\n",
      "Epoch 104, Train Loss: 57391.7321, Train RMSE: 15.15\n",
      "Epoch 105, Train Loss: 44582.5787, Train RMSE: 13.35\n",
      "Epoch 106, Train Loss: 42089.3343, Train RMSE: 12.98\n",
      "Epoch 107, Train Loss: 40272.4764, Train RMSE: 12.69\n",
      "Epoch 108, Train Loss: 40892.6936, Train RMSE: 12.79\n",
      "Epoch 109, Train Loss: 42068.5872, Train RMSE: 12.97\n",
      "Epoch 110, Train Loss: 43104.7813, Train RMSE: 13.13\n",
      "Epoch 111, Train Loss: 41555.2641, Train RMSE: 12.89\n",
      "Epoch 112, Train Loss: 40041.4196, Train RMSE: 12.66\n",
      "Epoch 113, Train Loss: 44460.4170, Train RMSE: 13.34\n",
      "Epoch 114, Train Loss: 38486.8371, Train RMSE: 12.41\n",
      "Epoch 115, Train Loss: 37627.5185, Train RMSE: 12.27\n",
      "Epoch 116, Train Loss: 36829.6811, Train RMSE: 12.14\n",
      "Epoch 117, Train Loss: 36375.6195, Train RMSE: 12.06\n",
      "Epoch 118, Train Loss: 36868.3142, Train RMSE: 12.14\n",
      "Epoch 119, Train Loss: 43951.4966, Train RMSE: 13.26\n",
      "Epoch 120, Train Loss: 37114.5020, Train RMSE: 12.18\n",
      "Epoch 121, Train Loss: 37068.5029, Train RMSE: 12.18\n",
      "Epoch 122, Train Loss: 35325.9797, Train RMSE: 11.89\n",
      "Epoch 123, Train Loss: 37317.7206, Train RMSE: 12.22\n",
      "Epoch 124, Train Loss: 35400.4587, Train RMSE: 11.90\n",
      "Epoch 125, Train Loss: 33544.2321, Train RMSE: 11.58\n",
      "Epoch 126, Train Loss: 34104.8600, Train RMSE: 11.68\n",
      "Epoch 127, Train Loss: 33549.2089, Train RMSE: 11.58\n",
      "Epoch 128, Train Loss: 38096.2689, Train RMSE: 12.34\n",
      "Epoch 129, Train Loss: 44543.2181, Train RMSE: 13.35\n",
      "Epoch 130, Train Loss: 34723.9499, Train RMSE: 11.79\n",
      "Epoch 131, Train Loss: 33681.8967, Train RMSE: 11.61\n",
      "Epoch 132, Train Loss: 31329.5658, Train RMSE: 11.19\n",
      "Epoch 133, Train Loss: 31301.1659, Train RMSE: 11.19\n",
      "Epoch 134, Train Loss: 31358.4735, Train RMSE: 11.20\n",
      "Epoch 135, Train Loss: 34071.2636, Train RMSE: 11.67\n",
      "Epoch 136, Train Loss: 33569.1737, Train RMSE: 11.59\n",
      "Epoch 137, Train Loss: 37144.2909, Train RMSE: 12.19\n",
      "Epoch 138, Train Loss: 31857.3250, Train RMSE: 11.29\n",
      "Epoch 139, Train Loss: 30662.4283, Train RMSE: 11.07\n",
      "Epoch 140, Train Loss: 28878.6394, Train RMSE: 10.75\n",
      "Epoch 141, Train Loss: 30225.0359, Train RMSE: 11.00\n",
      "Epoch 142, Train Loss: 29392.0841, Train RMSE: 10.84\n",
      "Epoch 143, Train Loss: 29711.1144, Train RMSE: 10.90\n",
      "Epoch 144, Train Loss: 30034.8658, Train RMSE: 10.96\n",
      "Epoch 145, Train Loss: 30244.7750, Train RMSE: 11.00\n",
      "Epoch 146, Train Loss: 28558.4530, Train RMSE: 10.69\n",
      "Epoch 147, Train Loss: 27575.4232, Train RMSE: 10.50\n",
      "Epoch 148, Train Loss: 28109.9892, Train RMSE: 10.60\n",
      "Epoch 149, Train Loss: 27586.6417, Train RMSE: 10.50\n",
      "Epoch 150, Train Loss: 27672.4027, Train RMSE: 10.52\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = LSTMModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(150):\n",
    "    model.train()\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        train_preds.append(pred.detach())\n",
    "        train_targets.append(yb)\n",
    "\n",
    "    train_preds = torch.cat(train_preds)\n",
    "    train_targets = torch.cat(train_targets)\n",
    "    train_rmse = math.sqrt(F.mse_loss(train_preds, train_targets).item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Train RMSE: {train_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "311ac6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 16.37\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for xb, yb in val_loader:\n",
    "        y_pred = model(xb)\n",
    "        preds.append(y_pred)\n",
    "        targets.append(yb)\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    rmse = math.sqrt(F.mse_loss(preds, targets).item())\n",
    "    print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "\n",
    "    #18.44 -> hidden_size=128, num_layers=3, dropout=0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76b324bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_val).squeeze().numpy()\n",
    "\n",
    "# Add prediction column and save\n",
    "test_df[\"carb\"] = preds\n",
    "test_df.to_csv(\"test_with_predictions_transformer_lstm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
